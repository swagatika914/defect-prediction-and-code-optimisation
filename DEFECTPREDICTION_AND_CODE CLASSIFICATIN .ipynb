{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3acd20-e45f-4e01-b150-ec7ce9e02e84",
   "metadata": {},
   "source": [
    "### Defect prediction using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572d3c08-534d-4210-bdd1-32c2670d7b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       119\n",
      "           1       0.48      0.48      0.48        50\n",
      "\n",
      "    accuracy                           0.69       169\n",
      "   macro avg       0.63      0.63      0.63       169\n",
      "weighted avg       0.69      0.69      0.69       169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"petersa2/CodeNet\", split=\"train\")\n",
    "\n",
    "# Prepare data: Get source code and correctness label\n",
    "# Use the 'text' column for code and 'label' for the status\n",
    "codes = [sample[\"text\"] for sample in dataset]\n",
    "labels = [sample[\"label\"] for sample in dataset]\n",
    "\n",
    "# Text vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(codes)\n",
    "y = labels\n",
    "\n",
    "# Split and train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba7454-b8ac-4f43-bd87-43497b129e5f",
   "metadata": {},
   "source": [
    "### Code Classification using LSTM(Long Short Term Memory)+PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7689e1f9-139b-42fd-a50c-e87df91e0e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.5401\n",
      "Epoch 2, Loss: 7.1646\n",
      "Epoch 3, Loss: 6.8648\n",
      "Epoch 4, Loss: 6.6971\n",
      "Epoch 5, Loss: 6.4959\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.97      0.82       119\n",
      "           1       0.43      0.06      0.11        50\n",
      "\n",
      "    accuracy                           0.70       169\n",
      "   macro avg       0.57      0.51      0.46       169\n",
      "weighted avg       0.63      0.70      0.61       169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load dataset\n",
    "dataset = load_dataset(\"petersa2/CodeNet\", split=\"train\")\n",
    "codes = [sample[\"text\"] for sample in dataset]\n",
    "labels = [sample[\"label\"] for sample in dataset]\n",
    "\n",
    "# Step 2: Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Step 3: Tokenize code manually\n",
    "def simple_tokenizer(code):\n",
    "    return code.lower().split()\n",
    "\n",
    "tokenized_codes = [simple_tokenizer(code) for code in codes]\n",
    "\n",
    "# Step 4: Build vocabulary\n",
    "from collections import Counter\n",
    "vocab = Counter(token for code in tokenized_codes for token in code)\n",
    "vocab = {token: idx+2 for idx, (token, _) in enumerate(vocab.most_common(10000))}\n",
    "vocab[\"<PAD>\"] = 0\n",
    "vocab[\"<UNK>\"] = 1\n",
    "\n",
    "# Step 5: Convert tokens to indices\n",
    "def encode(code, vocab, max_len=300):\n",
    "    tokens = [vocab.get(token, vocab[\"<UNK>\"]) for token in code]\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [vocab[\"<PAD>\"]] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "X = np.array([encode(code, vocab) for code in tokenized_codes])\n",
    "y = np.array(y)\n",
    "\n",
    "# Step 6: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Step 8: Define LSTM model\n",
    "class CodeClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(CodeClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.relu(self.fc1(hidden.squeeze(0)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Step 9: Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CodeClassifier(vocab_size=len(vocab), embed_dim=128, hidden_dim=64, output_dim=len(set(y)))\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 10: Train model\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Step 11: Evaluate\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch_y.numpy())\n",
    "\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f5c73-458e-49b1-8f3a-b853c9f85c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
